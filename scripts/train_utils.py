import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import math
import random

from scripts.data_augmentation import mixup_data, cutmix_data, mixup_cutmix_criterion


# ==============================================================
# 1. è®­ç»ƒå•ä¸ª epoch
# ==============================================================
def train_epoch(model, dataloader, criterion, optimizer, device, epoch, warmup_scheduler=None):
    """
    å•è½®è®­ç»ƒ
    Args:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        optimizer: ä¼˜åŒ–å™¨
        device: è¿è¡Œè®¾å¤‡ (cpu/cuda)
        epoch: å½“å‰ epoch ç¼–å·
        warmup_scheduler: å¯é€‰çš„ warm-up è°ƒåº¦å™¨
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch} [Training]", leave=False)

    for inputs, labels in progress_bar:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()

        # ----------------------------------------------------------
        # ğŸ”¹ éšæœºå¯ç”¨ Mixup æˆ– CutMixï¼ˆæ¦‚ç‡å„ 50%ï¼‰
        # ----------------------------------------------------------
        r = random.random()
        if r < 0.5:
            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=1.0, device=device)
            outputs = model(inputs)
            loss = mixup_cutmix_criterion(criterion, outputs, targets_a, targets_b, lam)
        else:
            inputs, targets_a, targets_b, lam = cutmix_data(inputs, labels, alpha=1.0, device=device)
            outputs = model(inputs)
            loss = mixup_cutmix_criterion(criterion, outputs, targets_a, targets_b, lam)

        # ----------------------------------------------------------
        # ğŸ”¹ åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
        # ----------------------------------------------------------
        loss.backward()
        optimizer.step()

        # ğŸ”¹ Warmup è°ƒåº¦ï¼ˆåœ¨å‰ 20 epoch å†…çº¿æ€§å¢é•¿ï¼‰
        if warmup_scheduler is not None:
            warmup_scheduler.step()

        # ----------------------------------------------------------
        # ğŸ”¹ ç»Ÿè®¡æŒ‡æ ‡
        # ----------------------------------------------------------
        running_loss += loss.item() * inputs.size(0)

        # Mixup / CutMix ä¸å…·å¤‡çœŸå®æ ‡ç­¾ï¼Œå› æ­¤å‡†ç¡®ç‡ä»…åœ¨æ™®é€šé˜¶æ®µç»Ÿè®¡
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        progress_bar.set_postfix({
            "Loss": f"{loss.item():.4f}",
            "Acc": f"{100.0 * correct / total:.2f}%"
        })

    epoch_loss = running_loss / total
    epoch_acc = 100.0 * correct / total
    return epoch_loss, epoch_acc


# ==============================================================
# 2. éªŒè¯é˜¶æ®µ
# ==============================================================
@torch.no_grad()
def validate_epoch(model, dataloader, criterion, device, epoch):
    """
    éªŒè¯é˜¶æ®µï¼šä¸ä½¿ç”¨ Mixup / CutMix
    """
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch} [Validation]", leave=False)

    for inputs, labels in progress_bar:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        running_loss += loss.item() * inputs.size(0)
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        progress_bar.set_postfix({
            "Loss": f"{loss.item():.4f}",
            "Acc": f"{100.0 * correct / total:.2f}%"
        })

    epoch_loss = running_loss / total
    epoch_acc = 100.0 * correct / total
    return epoch_loss, epoch_acc


# ==============================================================
# 3. ä¿å­˜æ¨¡å‹ Checkpoint
# ==============================================================
def save_checkpoint(model, optimizer, epoch, best_acc, path="checkpoint.pth"):
    state = {
        "model_state": model.state_dict(),
        "optimizer_state": optimizer.state_dict(),
        "epoch": epoch,
        "best_acc": best_acc,
    }
    torch.save(state, path)


# ==============================================================
# 4. å®šä¹‰ä¼˜åŒ–å™¨ä¸å­¦ä¹ ç‡è°ƒåº¦å™¨
# ==============================================================
def define_optimizer_and_scheduler(model, total_epochs=300, warmup_epochs=20, base_lr=0.1):
    """
    æ„å»º SGD ä¼˜åŒ–å™¨ + Warmup + Cosine Annealing è°ƒåº¦å™¨
    """
    optimizer = optim.SGD(
        model.parameters(),
        lr=base_lr,
        momentum=0.9,
        weight_decay=5e-4,
        nesterov=True
    )

    # Cosine Annealing: è®© lr ä» base_lr â†’ lr_min=1e-6
    cosine_scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=(total_epochs - warmup_epochs),
        eta_min=1e-6
    )

    # Linear Warmup Scheduler
    def lr_lambda(current_step):
        if current_step < warmup_epochs:
            return float(current_step) / float(max(1, warmup_epochs))
        else:
            progress = (current_step - warmup_epochs) / float(max(1, total_epochs - warmup_epochs))
            return 0.5 * (1.0 + math.cos(math.pi * progress))

    warmup_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    return optimizer, warmup_scheduler, cosine_scheduler
