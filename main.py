#!/usr/bin/env python3

import argparse
import logging
import os
import random
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets

from scripts.data_download import (
    download_and_extract_cifar10_data,
    download_and_extract_cifar100_data,
)
from scripts.data_augmentation import augment_dataset  # ç»Ÿä¸€å…¥å£
from scripts.model_architectures import create_model
from scripts.train_utils import (
    train_epoch,
    validate_epoch,
    save_checkpoint,
    define_optimizer_and_scheduler,
    compute_f1_scores
)
from scripts.evaluation_metrics import evaluate_model

# ===== Logging =====
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler("cifar_pipeline.log")],
)
logger = logging.getLogger(__name__)


def set_random_seeds(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.use_deterministic_algorithms(True, warn_only=True)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def parse_args():
    parser = argparse.ArgumentParser(description="CIFAR-10/100 Strong Baseline")

    # Dataset / IO
    parser.add_argument("--dataset", type=str, choices=["cifar10", "cifar100"], default="cifar100")
    parser.add_argument("--data_dir", type=str, default="data")
    parser.add_argument("--output_dir", type=str, default="results")

    # Hyperparams
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--num_epochs", type=int, default=300)
    parser.add_argument("--warmup_epochs", type=int, default=20)
    parser.add_argument("--lr", type=float, default=0.1)
    parser.add_argument("--weight_decay", type=float, default=5e-4)

    # Hardware
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--num_workers", type=int, default=4)

    # Misc
    parser.add_argument("--seed", type=int, default=42)

    # Offline augmentation switchï¼ˆä¿æŒåŸç»“æ„ï¼‰
    parser.add_argument("--offline_aug", action="store_true", help="Run disk-based augmentation before training")
    parser.add_argument("--offline_aug_times", type=int, default=1, help="Augmentations per image when offline")
    return parser.parse_args()


# --------------------------
# 1) Collect data
# --------------------------
def collect_data(args):
    os.makedirs(os.path.join(args.data_dir, "raw"), exist_ok=True)
    if args.dataset == "cifar10":
        download_and_extract_cifar10_data(root_dir=os.path.join(args.data_dir, "raw"))
    else:
        download_and_extract_cifar100_data(root_dir=os.path.join(args.data_dir, "raw"))
    logger.info("Data prepared under data/raw/train and data/raw/test.")


# --------------------------
# 2) Augment data (optional, keep structure)
# --------------------------
def augment_data(args):
    """
    ç»“æ„ä¿ç•™ï¼šå¦‚ --offline_aug æ‰“å¼€ï¼Œåˆ™å¯¹ raw åšç¦»çº¿å¢å¼ºåˆ° data/augmentedï¼›
    å¦åˆ™ä¸åšç¦»çº¿å¢å¼ºï¼ˆåœ¨çº¿å¢å¼ºä¼šåœ¨ build_dataloaders é‡Œé€šè¿‡è¿”å›çš„ transforms å®Œæˆï¼‰
    """
    if not args.offline_aug:
        logger.info("Skip offline augmentation (recommended). Use --offline_aug to enable.")
        return

    raw_dir = os.path.join(args.data_dir, "raw")
    aug_dir = os.path.join(args.data_dir, "augmented")
    os.makedirs(aug_dir, exist_ok=True)

    # è°ƒç”¨ç»Ÿä¸€å…¥å£ï¼šç¦»çº¿å¢å¼ºï¼ˆä»…å‡ ä½•/é¢œè‰²ï¼›ä¿ç•™åŸå›¾ï¼‰
    augment_dataset(
        input_dir=raw_dir,
        output_dir=aug_dir,
        augmentations_per_image=args.offline_aug_times,
        seed=args.seed,
        dataset=args.dataset,
        save_to_disk=True,
    )
    logger.info(f"Offline augmentation finished â†’ {aug_dir}")




# --------------------------
# åœ¨çº¿/ç¦»çº¿ä¸‹ï¼Œæ„å»º DataLoader
# --------------------------
def build_dataloaders(args):
    """
    - åœ¨çº¿å¢å¼ºï¼šä» augment_dataset(..., save_to_disk=False) æ‹¿åˆ° (train_tf, test_tf)
    - ç¦»çº¿å¢å¼ºï¼šä½ å¯ä»¥æŠŠ root æ”¹æˆ data/augmentedï¼›è¿™é‡Œä¿æŒé»˜è®¤ç”¨ raw
    """
    train_root = os.path.join(args.data_dir, "raw", "train")
    test_root = os.path.join(args.data_dir, "raw", "test")

    # åœ¨çº¿å¢å¼ºï¼ˆè¿”å› transformsï¼‰
    train_tf, test_tf = augment_dataset(
        input_dir=None, output_dir=None,  # åœ¨çº¿å¢å¼ºä¸éœ€è¦è¿™äº›
        augmentations_per_image=1, seed=args.seed,
        dataset=args.dataset, save_to_disk=False
    )

    train_set = datasets.ImageFolder(root=train_root, transform=train_tf)
    val_set = datasets.ImageFolder(root=test_root, transform=test_tf)

    train_loader = DataLoader(
        train_set, batch_size=args.batch_size, shuffle=True,
        num_workers=args.num_workers, pin_memory=True
    )
    val_loader = DataLoader(
        val_set, batch_size=args.batch_size, shuffle=False,
        num_workers=args.num_workers, pin_memory=True
    )
    return train_loader, val_loader, train_set.classes


# --------------------------
# 3) Build model
# --------------------------
def build_model(args):
    num_classes = 10 if args.dataset == "cifar10" else 100
    model = create_model(num_classes=num_classes, device=args.device)
    return model


# --------------------------
# 4) Train
# --------------------------
def train(args, model):
    train_loader, val_loader, class_names = build_dataloaders(args)

    criterion = nn.CrossEntropyLoss()
    optimizer, scheduler = define_optimizer_and_scheduler(
        model,
        total_epochs=args.num_epochs,
        warmup_epochs=args.warmup_epochs,
        base_lr=args.lr,
        weight_decay=args.weight_decay,
    )

    best_acc = 0.0
    os.makedirs(os.path.join(args.output_dir, "models"), exist_ok=True)

    for epoch in range(1, args.num_epochs + 1):
        tr_loss, tr_acc = train_epoch(
        model, train_loader, criterion, optimizer, args.device, epoch
        )
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, args.device, epoch)

        scheduler.step()

        logging.info(
        f"Epoch {epoch:03d}/{args.num_epochs} | "
        f"Train Loss {tr_loss:.4f} Acc {tr_acc:.2f}% | "
        f"Val Loss {val_loss:.4f} Acc {val_acc:.2f}%"
        )

        if val_acc > best_acc:
            best_acc = val_acc
            save_checkpoint(
                model, optimizer, epoch, best_acc,
                path=os.path.join(args.output_dir, "models", "best.pth")
            )
            logging.info(f"  â†³ New best Acc {best_acc:.2f}%, checkpoint saved.")

    return model, val_loader, class_names, best_acc


# --------------------------
# 5) Evaluate
# --------------------------
def evaluate(args, model):
    _, val_loader, class_names = build_dataloaders(args)
    criterion = nn.CrossEntropyLoss()
    loss, acc, all_preds, all_labels, all_probs = evaluate_model(model, val_loader, criterion, args.device)

    # ğŸ”¹ è®¡ç®—å¹¶æ‰“å° F1ï¼ˆmacro/micro/weightedï¼‰
    num_classes = len(class_names)
    f1_macro, f1_micro, f1_weighted = compute_f1_scores(all_labels, all_preds, num_classes)

    logging.info(
        f"[Test] Loss {loss:.4f} Acc {acc:.2f}% | "
        f"F1(macro) {f1_macro:.4f} | F1(micro) {f1_micro:.4f} | F1(weighted) {f1_weighted:.4f}"
    )



def main():
    args = parse_args()
    set_random_seeds(args.seed)

    logger.info("===== Strong CIFAR Baseline Start =====")
    for k, v in vars(args).items():
        logger.info(f"{k}: {v}")

    collect_data(args)
    augment_data(args)  # å¯é€‰ï¼ˆç¦»çº¿ï¼‰
    model = build_model(args)
    model, val_loader, class_names, best_acc = train(args, model)
    evaluate(args, model)

    logger.info(f"Done. Best Val/Test Acc: {best_acc:.2f}%")


if __name__ == "__main__":
    main()
