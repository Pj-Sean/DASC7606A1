#!/usr/bin/env python3

import argparse
import logging
import os
import random
import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets

from scripts.data_download import (
    download_and_extract_cifar10_data,
    download_and_extract_cifar100_data,
)
from scripts.data_augmentation import augment_dataset  # Áªü‰∏ÄÂÖ•Âè£
from scripts.model_architectures import create_model
from scripts.train_utils import (
    train_epoch,
    validate_epoch,
    save_checkpoint,
    define_optimizer_and_scheduler,
)
from scripts.evaluation_metrics import evaluate_model

# ===== Logging =====
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(), logging.FileHandler("cifar_pipeline.log")],
)
logger = logging.getLogger(__name__)


def set_random_seeds(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.use_deterministic_algorithms(True, warn_only=True)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def parse_args():
    parser = argparse.ArgumentParser(description="CIFAR-10/100 Strong Baseline")

    # Dataset / IO
    parser.add_argument("--dataset", type=str, choices=["cifar10", "cifar100"], default="cifar100")
    parser.add_argument("--data_dir", type=str, default="data")
    parser.add_argument("--output_dir", type=str, default="results")

    # Hyperparams
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--num_epochs", type=int, default=300)
    parser.add_argument("--warmup_epochs", type=int, default=20)
    parser.add_argument("--lr", type=float, default=0.1)
    parser.add_argument("--weight_decay", type=float, default=5e-4)

    # Hardware
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--num_workers", type=int, default=4)

    # Misc
    parser.add_argument("--seed", type=int, default=42)

    # Offline augmentation switchÔºà‰øùÊåÅÂéüÁªìÊûÑÔºâ
    parser.add_argument("--offline_aug", action="store_true", help="Run disk-based augmentation before training")
    parser.add_argument("--offline_aug_times", type=int, default=1, help="Augmentations per image when offline")
    return parser.parse_args()


# --------------------------
# 1) Collect data
# --------------------------
def collect_data(args):
    os.makedirs(os.path.join(args.data_dir, "raw"), exist_ok=True)
    if args.dataset == "cifar10":
        download_and_extract_cifar10_data(root_dir=os.path.join(args.data_dir, "raw"))
    else:
        download_and_extract_cifar100_data(root_dir=os.path.join(args.data_dir, "raw"))
    logger.info("Data prepared under data/raw/train and data/raw/test.")


# --------------------------
# 2) Augment data (optional, keep structure)
# --------------------------
def augment_data(args):
    """
    ÁªìÊûÑ‰øùÁïôÔºöÂ¶Ç --offline_aug ÊâìÂºÄÔºåÂàôÂØπ raw ÂÅöÁ¶ªÁ∫øÂ¢ûÂº∫Âà∞ data/augmentedÔºõ
    Âê¶Âàô‰∏çÂÅöÁ¶ªÁ∫øÂ¢ûÂº∫ÔºàÂú®Á∫øÂ¢ûÂº∫‰ºöÂú® build_dataloaders ÈáåÈÄöËøáËøîÂõûÁöÑ transforms ÂÆåÊàêÔºâ
    """
    if not args.offline_aug:
        logger.info("Skip offline augmentation (recommended). Use --offline_aug to enable.")
        return

    raw_dir = os.path.join(args.data_dir, "raw")
    aug_dir = os.path.join(args.data_dir, "augmented")
    os.makedirs(aug_dir, exist_ok=True)

    # Ë∞ÉÁî®Áªü‰∏ÄÂÖ•Âè£ÔºöÁ¶ªÁ∫øÂ¢ûÂº∫Ôºà‰ªÖÂá†‰Ωï/È¢úËâ≤Ôºõ‰øùÁïôÂéüÂõæÔºâ
    augment_dataset(
        input_dir=raw_dir,
        output_dir=aug_dir,
        augmentations_per_image=args.offline_aug_times,
        seed=args.seed,
        dataset=args.dataset,
        save_to_disk=True,
    )
    logger.info(f"Offline augmentation finished ‚Üí {aug_dir}")


# --------------------------
# Âú®Á∫ø/Á¶ªÁ∫ø‰∏ãÔºåÊûÑÂª∫ DataLoader
# --------------------------
def build_dataloaders(args):
    """
    - Âú®Á∫øÂ¢ûÂº∫Ôºö‰ªé augment_dataset(..., save_to_disk=False) ÊãøÂà∞ (train_tf, test_tf)
    - Á¶ªÁ∫øÂ¢ûÂº∫Ôºö‰Ω†ÂèØ‰ª•Êää root ÊîπÊàê data/augmentedÔºõËøôÈáå‰øùÊåÅÈªòËÆ§Áî® raw
    """
    train_root = os.path.join(args.data_dir, "raw", "train")
    test_root = os.path.join(args.data_dir, "raw", "test")

    # Âú®Á∫øÂ¢ûÂº∫ÔºàËøîÂõû transformsÔºâ
    train_tf, test_tf = augment_dataset(
        input_dir=None, output_dir=None,  # Âú®Á∫øÂ¢ûÂº∫‰∏çÈúÄË¶ÅËøô‰∫õ
        augmentations_per_image=1, seed=args.seed,
        dataset=args.dataset, save_to_disk=False
    )

    train_set = datasets.ImageFolder(root=train_root, transform=train_tf)
    val_set = datasets.ImageFolder(root=test_root, transform=test_tf)

    train_loader = DataLoader(
        train_set, batch_size=args.batch_size, shuffle=True,
        num_workers=args.num_workers, pin_memory=True
    )
    val_loader = DataLoader(
        val_set, batch_size=args.batch_size, shuffle=False,
        num_workers=args.num_workers, pin_memory=True
    )
    return train_loader, val_loader, train_set.classes


# --------------------------
# 3) Build model
# --------------------------
def build_model(args):
    num_classes = 10 if args.dataset == "cifar10" else 100
    model = create_model(num_classes=num_classes, device=args.device)
    return model


# --------------------------
# 4) Train
# --------------------------
def train(args, model):
    train_loader, val_loader, class_names = build_dataloaders(args)

    criterion = nn.CrossEntropyLoss()
    optimizer, scheduler = define_optimizer_and_scheduler(
        model,
        total_epochs=args.num_epochs,
        warmup_epochs=args.warmup_epochs,
        base_lr=args.lr,
        weight_decay=args.weight_decay,
    )

    best_acc = 0.0
    os.makedirs(os.path.join(args.output_dir, "models"), exist_ok=True)

    for epoch in range(1, args.num_epochs + 1):
        tr_loss, tr_acc = train_epoch(
            model, train_loader, criterion, optimizer, args.device, epoch
        )
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, args.device, epoch)

        # üîπ ÊØè‰∏™ epoch ÁªìÊùüÂêéË∞ÉÂ∫¶‰∏ÄÊ¨°ÔºàÈÅøÂÖç batch Á∫ß stepÔºâ
        scheduler.step()

        logging.info(
            f"Epoch {epoch:03d}/{args.num_epochs} | "
            f"Train Loss {tr_loss:.4f} Acc {tr_acc:.2f}% | "
            f"Val Loss {val_loss:.4f} Acc {val_acc:.2f}%"
        )

        if val_acc > best_acc:
            best_acc = val_acc
            save_checkpoint(
                model, optimizer, epoch, best_acc,
                path=os.path.join(args.output_dir, "models", "best.pth")
            )
            logging.info(f"  ‚Ü≥ New best Acc {best_acc:.2f}%, checkpoint saved.")

    return model, val_loader, class_names, best_acc


# --------------------------
# 5) Evaluate
# --------------------------
def evaluate(args, model):
    _, val_loader, class_names = build_dataloaders(args)
    criterion = nn.CrossEntropyLoss()
    loss, acc, all_preds, all_labels, all_probs = evaluate_model(model, val_loader, criterion, args.device)
    logging.info(f"[Test] Loss {loss:.4f} Acc {acc:.2f}%")


def main():
    args = parse_args()
    set_random_seeds(args.seed)

    logger.info("===== Strong CIFAR Baseline Start =====")
    for k, v in vars(args).items():
        logger.info(f"{k}: {v}")

    collect_data(args)
    augment_data(args)  # ÂèØÈÄâÔºàÁ¶ªÁ∫øÔºâ
    model = build_model(args)
    model, val_loader, class_names, best_acc = train(args, model)
    evaluate(args, model)

    logger.info(f"Done. Best Val/Test Acc: {best_acc:.2f}%")


if __name__ == "__main__":
    main()
